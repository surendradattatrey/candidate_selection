{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capstone Project -  E-Recruitment | S&P Global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sponsor:-  \n",
    " - Dattatrey, Surendra \n",
    " \n",
    "### Mentor:-  \n",
    " - Prof. S M Bendre\n",
    "\n",
    "### Team Members:- \n",
    "    - Devender Bansal ID:- 11810089\n",
    "    - Vikash Singh Negi-- 11810048\n",
    "    - Sandeep Kumar Singh - 11810014\n",
    "    - Atul Sharma - 11810126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mammoth\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "import nltk, string\n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import subprocess\n",
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import HTMLConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"c://CBAWDR//rec//\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "docfiles=[]\n",
    "pdffiles=[]\n",
    "Unsupported=[]\n",
    "def file_seg(s):\n",
    "    flist=os.listdir(s)\n",
    "    for file in flist:\n",
    "        filename=file.split('.')\n",
    "        if filename[1] == 'docx':\n",
    "            docfiles.append(file)\n",
    "        elif filename[1]=='pdf':\n",
    "            pdffiles.append(file)\n",
    "        else:\n",
    "            msg=\"Unsupported format\"\n",
    "            Unsupported.append(msg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanpara(s):\n",
    "    clean = re.compile('<.*?>')\n",
    "    s = re.sub(clean, '', s)\n",
    "    return s\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=[]\n",
    "def paras_doc(s):    \n",
    "    for file in docfiles:\n",
    "        new_path= path + str(file)\n",
    "        with open(new_path, \"rb\") as doc_file:\n",
    "            result = mammoth.convert_to_html(doc_file)\n",
    "            html = result.value \n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            article=soup.text\n",
    "            para= sent_tokenize(article)\n",
    "            for par in para:\n",
    "                cleanpara(par)\n",
    "                paragraph.append(par)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paras_pdf(s):\n",
    "    for file in pdffiles:\n",
    "        new_path= path + str(file)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        retstr = BytesIO()\n",
    "        codecs = 'utf-8'\n",
    "        laparams = LAParams()\n",
    "        device = HTMLConverter(rsrcmgr, retstr,codec=codecs,  laparams=laparams)\n",
    "        fp = open(new_path, 'rb')\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        password = \"\"\n",
    "        maxpages = 0 #is for all\n",
    "        caching = True\n",
    "        pagenos=set()\n",
    "        for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "            interpreter.process_page(page)\n",
    "        fp.close()\n",
    "        device.close()\n",
    "        html = retstr.getvalue()\n",
    "        retstr.close()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        article=soup.text\n",
    "        para= sent_tokenize(article)\n",
    "        for par in para:\n",
    "            cleanpara(par)\n",
    "            paragraph.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=[]\n",
    "def paragphs(s):\n",
    "    file_seg(s)\n",
    "    paras_doc(s)\n",
    "    paras_pdf(s)\n",
    "    para_df=pd.DataFrame(paragraph, columns=['Text'])\n",
    "    para_df['Segment'] = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD_Para=[]\n",
    "def jd_para(s):\n",
    "    with open(s, \"rb\") as doc_file:\n",
    "        result = mammoth.convert_to_html(doc_file)\n",
    "        html = result.value \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        article=soup.text\n",
    "        para= sent_tokenize(article)\n",
    "        for par in para:\n",
    "            cleanpara(par)\n",
    "            JD_Para(par)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragphs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"c:\\CBAWDR\\Segments _Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 3', 'Sn.No'] ,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_value=list(data['Segment'].drop_duplicates())\n",
    "int_value=list(range(1,7))\n",
    "annotation_id=dict(zip(annotation_value,int_value))\n",
    "rev_annotation= dict(zip(int_value,annotation_value))\n",
    "data['Id']=data['Segment'].map(annotation_id)\n",
    "data=data.drop(columns=['Segment'])\n",
    "data.dropna(how ='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization, removal of stop words, removal of low frequency(3%) & High Frequency (97%) words. Encoding & creation of DTM\n",
    "vectorizer=CountVectorizer(min_df = 0.03 , max_df= 0.97, stop_words='english')\n",
    "vectorizer.fit(data.iloc[:,0])\n",
    "dtm=vectorizer.transform(data.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Feature & label and splitting the data into training(70%) & testing(30%)\n",
    "X,y=dtm,data.iloc[:,-1]\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "#Train the classifier(Logistical Regression) with Training Data. Predicting the Label for test Data & Score the model on Accuracy\n",
    "clf=LogisticRegression(random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "y_predict=clf.predict(X_test)\n",
    "score_Log=clf.score(X_test,y_test)\n",
    "print(score_Log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_path=\"c://CBAWDR//resume//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=[]\n",
    "def dfm(list):\n",
    "    df=pd.DataFrame(list, columns=['Text'])      \n",
    "    label_para=vectorizer.transform(df.iloc[:,0])\n",
    "    pred_label=clf.predict(label_para)\n",
    "    predict.extend(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "docfiles=[]\n",
    "pdffiles=[]\n",
    "Unsupported=[]\n",
    "paragraph=[]\n",
    "paragphs(resume_path)\n",
    "dfm(paragraph)\n",
    "df_segment= pd.DataFrame(list(zip(paragraph, predict)), \n",
    "               columns =['Text', 'label']) \n",
    "rev_annotation= dict(zip(int_value,annotation_value))\n",
    "df_segment['Segment']=df_segment['label'].map(rev_annotation)\n",
    "df_segment.drop(['label'] ,axis=1, inplace=True)\n",
    "segmentation=df_segment.groupby('Segment').agg({ 'Text': lambda x: ' '.join(x)}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_predict=[]\n",
    "def dfm(list):\n",
    "    df=pd.DataFrame(list, columns=['Text'])      \n",
    "    label_para=vectorizer.transform(df.iloc[:,0])\n",
    "    pred_label=clf.predict(label_para)\n",
    "    jd_predict.extend(pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segment_jd= pd.DataFrame(list(zip(JD_Para, jd_predict)), \n",
    "               columns =['Text', 'label']) \n",
    "rev_annotation= dict(zip(int_value,annotation_value))\n",
    "df_segment_jd['Segment']=df_segment_jd['label'].map(rev_annotation)\n",
    "df_segment_jd.drop(['label'] ,axis=1, inplace=True)\n",
    "segmentation_jd=df_segment_jd.groupby('Segment').agg({ 'Text': lambda x: ' '.join(x)}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color: \n",
    "UNDERLINE = '\\033[4m'\n",
    "END = '\\033[0m'\n",
    "k= segmentation_jd.shape[0]  \n",
    "for i in range(k):\n",
    "    print(color.BOLD + color.UNDERLINE + segmentation_jd.iloc[i,0] + color.END  + color.END)\n",
    "    print(segmentation_jd.iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mExperience_Skills\u001b[0m\u001b[0m\n",
      "Approx.Current Role\t:\tSenior Technical Account ManagerTotal year in Premier Business: 9+Contribution to Business Growth & CPECurrently Managing strategic high-visibility and high-touch customer like HCL, HP and Genpact Distinction of Managing Large number of customer in different verticals in different parts of India e.g. IT Roadmap & ProjectsIT OperationsIT GovernanceVendor Management Lifecycle Management for Company Portal & Website www.softwaredioxide.com  ,  www.qaiindia.com  Highlights Undertook & Completed IT projects on Capacity Enhancement. (Consultancy Management) from Birla Institute of Technology and Science (BITS), Pilani.Electronics and Telecommunication Engineer from the Institution of Electronics and Telecommunication Engineers, New Delhi.PERSONAL DETAILS  DATE OF BIRTH\t:\t29TH DEC. 1976.ADDRESS\t:\tC-201, Sanghamitra Apartments, Sector 4, Plot no.\n",
      "\u001b[1m\u001b[4mOther\u001b[0m\u001b[0m\n",
      "Devender BansalAlias: debansal, Ph. : 09873184914Total Work Experience\t:\t15 Yrs. Continuous Improvement in Satisfaction Index every year at customersPart of the PSS ELT (Extended Leadership Team)List of Customers Worked with in MS Premier CareerHCL\tGenpact\tHP Global TS\tJKB\tEXL\tAdobe\tIBM Daksh\tSaharaKPMG\tPepsi\tITC Limited\t  Tata Tea\tLinde\tRS Software\tDassault SoftwareBajaj Auto\tKuoni Travels\tKVB\tMurugappa Group\tCITOS\tSCB\tRAMCOTata Steel\tNIIT Ltd.\tNIIT Tech\tPolaris\t\tNDPL\tHT MediaCovansys\tInnodata Initiatives – Led multiple initiatives in MS Premier in last 4 YearsLed Country Initiative for SAB Conversion for 2 Mil Revenue in FY09Led Regional Initiative for Developing Calcutta region for Premier Business in FY10Led Global Customer Initiave for North to enhance revenue from Allocation Customers in FY11Led Regional Initiave for North in Premier Value Positioning in FY1Contribution to Delivery ManagementExecuted PSDM approach for Delivery Management at important customer SPOC for North for GTSC & Premier Rhythm SPOC for North for Critsit Team & Premier RhythmRepresenting North team on Global Multiyear Project Premier LeapNorth SPOC in KRA discussion for TAMs TeamContribution to TeamMentored New Joinees on Customer Handling Mentored Peers on Positioning of Value Add OfferingsMentored Peers on Value Based Customer DiscussionsAssisted In Account Distribution planningMentored Candidates on TAM roleOn Interview panel for TAM recruitmentPerformanceContinuous strong Performance over last Six years. Duration\t:\tJune 2003 to March 2005Designation\t:\tSr. Exchange AdministratorKey Responsibility Areas \t  L3 Problem management Support for Directory services and Messaging infrastructure ofComputer Science Corporations (CSC) Client remotelyCustomer Satisfaction IndexSLA ComplianceClient ReviewsDesigning & Implementing Service Improvement AreasKT & Support TransitionsHighlightsEstablished as Premier Technical resource. Feature Enhancement and Cost RationalizationStructured cabling for QAI Head OfficePower redundancy for QAI Head OfficeVoIP telephony for Domestic Point to Point & International Point to Many Server & Vendor Consolidations for Mail server & Company WebsiteHardware standardization across the Head Office  Created process forBackup & Disaster Recovery for Critical Infrastructure ServiceMonthly User Survey for user Satisfaction IndexTicketing System & SLAs for Internal Users Created Blue book of IT as policy & guidelines forOrganizations Email policyOrganizations Internet browsing policyAsset Issue PolicyProcurement Criterion & Process for IT assetsAccomplished the above in just 11 months of employment with a team of 3 people Organization\t:\tITLESL  Duration\t:\tJuly 2000 to June 2002 Designation\t:\tSenior Executive TechnicalKey Responsibility Areas \t Managing IT Operations at the corporate office & Centers Vendor Management for Hardware, Software & Network RequirementsLicensing Compliance managementFranchise IT Infrastructure Sizing & ValidationChange Management for Software Deployments in Different Centers Consultant for ITL Sister Concern on IT Operations\\Deployments\\IT RecruitmentsHighlightsBuilt IT Infrastructure for Six state of Art Educational Centers, ground upImplemented E-Governance Project for ITL in the state of SIKKIM. Organization\t:\tOnline InfoTech Duration\t:\tSept 98 to May 2000Designation\t:    Customer support engineerKey Responsibility Areas\t: Supporting company’s customer on IT infrastructure issues server, desktop & Hardware. 20, Dwarka, New Delhi – 110078, (DEVENDER BANSAL)\n",
      "\u001b[1m\u001b[4mPersonal Details\u001b[0m\u001b[0m\n",
      "Rated Exceeded four times (Exceed & 1 Rating) in last five yearsRated 20% Twice on Contribution ranking Three Promotion in last 6 YearsNo Underperform rating in entire Microsoft CareerAwards & RecognitionsSegment Award in Service at annual Company Meeting Two Times GPGP AwardGold Star AwardAwarded Certificate of Recognition in ThinkWeekCo Delivered On Value Triangle Methodology in TechReadyProfessional Trainings and CertificationsProject management Professional (PMP) CertifiedCertified Information Systems Security Professional (CISSP)Certified Internal Information Security Auditor from STQC (CIISA)Six Sigma Green Belt CertifiedITIL 3.0 Foundation CertifiedITIL 2.0 Foundation CertifiedCertified MCSE for NT 4.0Prior Employment & Experience  Organization\t:\tCSC India.\n",
      "\u001b[1m\u001b[4mSkills\u001b[0m\u001b[0m\n",
      "Delhi NCR, Calcutta, Mumbai, Pune, BangaloreSuccessful in growing multiple accounts from thousand dollars to hundred thousand dollar like Genpact, Adobe, Pepsi, HP Etc.Successful in Selling through contracts with MS partner like HCLConsistently Overachieved Revenue & Customer Satisfaction Goals Scored high CPE over all the years. Achieved High ratings in both the yearsExceeded Customer Satisfaction Index In most AccountsAwarded for Innovation in Tackling satisfaction issue in new transitioned account Assisted as Internal Auditor towards BS7799 Certification Successfully Led the Six Sigma Project on Ticket defectsPart of Interview Panel for new recruitments  Organization\t:\tQAI (INDIA) Ltd. Duration\t:\tJuly 2002 to June 2003Designation\t:\tSenior System AdministratorKey Responsibility Areas \t:  Providing Leadership to the IT Corp Team of QAI with Head office In Delhi & Branch Offices at Bangalore, Bombay, and Hyderabad & China. Supporting Customer in Deployments including Messaging, Directory services, security products, hardware Servers and devices.HighlightsQuickly Learned & Promoted to Server Support & DeploymentEDUCATIONAL QUALIFICATIONSM.S.\n"
     ]
    }
   ],
   "source": [
    "class color: \n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "k= segmentation.shape[0]   \n",
    "\n",
    "for i in range(k):\n",
    "    print(color.BOLD + color.UNDERLINE + segmentation.iloc[i,0] + color.END  + color.END)\n",
    "    print(segmentation.iloc[i,1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data=segmentation.loc[(segmentation['Segment']== 'Experience') | (segmentation['Segment']== 'Experience_Skills')]\n",
    "exp_list=exp_data['Text'].tolist()\n",
    "exp_test=\" \".join(exp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx.Current Role\t:\tSenior Technical Account ManagerTotal year in Premier Business: 9+Contribution to Business Growth & CPECurrently Managing strategic high-visibility and high-touch customer like HCL, HP and Genpact Distinction of Managing Large number of customer in different verticals in different parts of India e.g. IT Roadmap & ProjectsIT OperationsIT GovernanceVendor Management Lifecycle Management for Company Portal & Website www.softwaredioxide.com  ,  www.qaiindia.com  Highlights Undertook & Completed IT projects on Capacity Enhancement. (Consultancy Management) from Birla Institute of Technology and Science (BITS), Pilani.Electronics and Telecommunication Engineer from the Institution of Electronics and Telecommunication Engineers, New Delhi.PERSONAL DETAILS  DATE OF BIRTH\t:\t29TH DEC. 1976.ADDRESS\t:\tC-201, Sanghamitra Apartments, Sector 4, Plot no.\n"
     ]
    }
   ],
   "source": [
    "print(exp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_text=exp_data.iloc[0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_extractor(s):\n",
    "    if re.search('experience', s):\n",
    "        for sen in (s).split('.'):    \n",
    "            if re.search('experience', sen):\n",
    "                if re.search('overall | total', sen):\n",
    "                    sen_tokenized=nltk.word_tokenize(sen)        \n",
    "                    tagged=nltk.pos_tag(sen_tokenized)\n",
    "                    entities = nltk.chunk.ne_chunk(tagged)\n",
    "                    for subtree in entities.subtrees():\n",
    "                        for leaf in subtree.leaves():\n",
    "                            if leaf[1] =='CD':\n",
    "                                experience=leaf[0]\n",
    "                                if experience !=\"\":                            \n",
    "                                    tot_exp.append(experience)\n",
    "                                else:                            \n",
    "                                    tot_exp.append('Not Found')\n",
    "    else:\n",
    "        tot_exp.append('Not Found')\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_exp=[]\n",
    "exp_extractor(exp_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Not Found']\n"
     ]
    }
   ],
   "source": [
    "print(tot_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_df_Resume=segmentation.loc[(segmentation['Segment']== 'Skills') | (segmentation['Segment']== 'Experience_Skills')]\n",
    "para_df_JD=segmentation_jd.loc[(segmentation_jd['Segment']== 'Skills') | (segmentation_jd['Segment']== 'Experience_Skills')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NLTK work and sentence tokinizer\n",
    "skill_level=[]\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "para_df_Resume['Segments'] = para_df_Resume.Text.map(sent_tokenize)\n",
    "para_df_Resume['tokens_sentences'] = para_df_Resume['Segments'].map(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "para_df_JD['Segments'] = para_df_JD.Text.map(sent_tokenize)\n",
    "para_df_JD['tokens_sentences'] = para_df_JD['Segments'].map(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "\n",
    "para_df_Resume['tokens_sentences']\n",
    "para_df_JD['tokens_sentences']\n",
    "\n",
    "## NLTK postagging\n",
    "\n",
    "from nltk import pos_tag\n",
    "para_df_Resume['POS_tokens'] = para_df_Resume['tokens_sentences'].map(lambda tokens_sentences: \n",
    "                                                           [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "para_df_JD['POS_tokens'] = para_df_JD['tokens_sentences'].map(lambda tokens_sentences: \n",
    "                                                           [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "\n",
    "#identifying only nouns\n",
    "# for sentence in data['articles']:\n",
    "###     data['nouns'] = [token for token, pos in pos_tag(word_tokenize(sentence)) if pos.startswith('N')]\n",
    "\n",
    "## NLTK lemmatizing and pos tagging\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizing each word with its POS tag, in each sentence\n",
    "para_df_Resume['tokens_sentences_lemmatized'] = para_df_Resume['POS_tokens'].map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")\n",
    "\n",
    "para_df_JD['tokens_sentences_lemmatized'] = para_df_JD['POS_tokens'].map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")\n",
    "\n",
    "# stop words\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'like', 'make', 'see', 'want', 'come', 'take', 'use', 'would', 'can']\n",
    "#stopwords_other = ['one', 'mr', 'bbc', 'image', 'getty', 'de', 'en', 'caption', 'also', 'copyright', 'something']\n",
    "my_stopwords = stopwords.words('English') #+ stopwords_verbs + stopwords_other\n",
    "\n",
    "from itertools import chain\n",
    "para_df_Resume['tokens'] = para_df_Resume['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "para_df_Resume['tokens'] = para_df_Resume['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>2])\n",
    "\n",
    "para_df_JD['tokens'] = para_df_JD['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "para_df_JD['tokens'] = para_df_JD['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>2])\n",
    "\n",
    "\n",
    "para_df_Resume['tokens']\n",
    "\n",
    "## looking for by grams and tri grams\n",
    "\n",
    "from gensim.models import Phrases\n",
    "nouns= []\n",
    "tokens = para_df_Resume['tokens'].tolist()\n",
    "# for tok in tokens:\n",
    "#     #print(pos_tag(tok))\n",
    "#     nouns.append([token for token, pos in pos_tag(tok) if pos.startswith('N')])\n",
    "# nouns = list(filter(None, nouns))\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])\n",
    "tokens = list(filter(None, tokens))\n",
    "\n",
    "\n",
    "tokens1 = para_df_JD['tokens'].tolist()\n",
    "# for tok in tokens:\n",
    "#     #print(pos_tag(tok))\n",
    "#     nouns.append([token for token, pos in pos_tag(tok) if pos.startswith('N')])\n",
    "# nouns = list(filter(None, nouns))\n",
    "bigram_model1 = Phrases(tokens1)\n",
    "trigram_model1 = Phrases(bigram_model[tokens1], min_count=1)\n",
    "tokens1 = list(trigram_model[bigram_model[tokens1]])\n",
    "tokens1 = list(filter(None, tokens1))\n",
    "\n",
    "tokens1\n",
    "\n",
    "#### unique words list\n",
    "unique_list = [] \n",
    "for x in tokens:\n",
    "    for y in x:\n",
    "        if y not in unique_list: \n",
    "            unique_list.append(y)\n",
    "unique_list = [w.replace('_', '') for w in unique_list]\n",
    "len(unique_list)\n",
    "\n",
    "\n",
    "#JD\n",
    "unique_listJD = [] \n",
    "for x in tokens1:\n",
    "    for y in x:\n",
    "        if y not in unique_listJD: \n",
    "            unique_listJD.append(y)\n",
    "unique_listJD = [w.replace('_', '') for w in unique_listJD]\n",
    "len(unique_listJD)\n",
    "\n",
    "def chk_lst(wrd,lst):\n",
    "    flg=False\n",
    "    for i in lst:\n",
    "        if re.search(wrd,i):\n",
    "            flg=True\n",
    "            return flg\n",
    "    return flg\n",
    "        \n",
    "\n",
    "def getmetrics(inp, result):\n",
    "    # Regexp match with any string in the list\n",
    "#     tn = len([value for value in inp if chk_lst(value,result)])\n",
    "    tp = len([value for value in inp if chk_lst(value,result)])\n",
    "    fp = len([value for value in inp if not chk_lst(value,result)])\n",
    "    fn = len([value for value in result if not chk_lst(value,inp)])\n",
    "    tp_val = [value for value in inp if chk_lst(value,result)]\n",
    "    print(\"True Positives - %s, False Positives - %s, False Negatives - %s\"%(tp,fp,fn))\n",
    "    if tp>0:\n",
    "#         accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "    else:\n",
    "#         accuracy=0\n",
    "        precision=0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "    return precision,recall,f1,tp_val\n",
    "\n",
    "# Evaluation of Only nltk Tokens - Model 1\n",
    "unique_list_low = [i.lower() for i in unique_list]\n",
    "unique_listJD_low = [i.lower() for i in unique_listJD]\n",
    "m1_precision, m1_recall, m1_f1, mat_val_m1 = getmetrics(unique_listJD_low, unique_list_low)\n",
    "print(\"Model 1 Precision - %s \\nModel 1 Recall - %s \\nModel 1 F1 - %s\"%( m1_precision,m1_recall,m1_f1))\n",
    "\n",
    "if (m1_precision >= 0.3 and float(m1_f1) >= 0.3) :\n",
    "    print(\"Best\")\n",
    "    skill_level.append('High')\n",
    "elif (m1_precision >= 0.2 and float(m1_f1) >= 0.2) :\n",
    "    print(\"Good\")\n",
    "    skill_level.append('Moderate')\n",
    "else:\n",
    "    print(\"Nakli\")\n",
    "    skill_level.append('Low')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_df_Resume_exp=exp_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Rel_Exp=[]\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "para_df_Resume_exp['Segments'] = para_df_Resume_exp.Text.map(sent_tokenize)\n",
    "para_df_Resume_exp['tokens_sentences'] = para_df_Resume_exp['Segments'].map(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "para_df_JD['Segments'] = para_df_JD.Text.map(sent_tokenize)\n",
    "para_df_JD['tokens_sentences'] = para_df_JD['Segments'].map(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "\n",
    "para_df_Resume_exp['tokens_sentences']\n",
    "para_df_JD['tokens_sentences']\n",
    "\n",
    "## NLTK postagging\n",
    "\n",
    "from nltk import pos_tag\n",
    "para_df_Resume_exp['POS_tokens'] = para_df_Resume_exp['tokens_sentences'].map(lambda tokens_sentences: \n",
    "                                                           [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "para_df_JD['POS_tokens'] = para_df_JD['tokens_sentences'].map(lambda tokens_sentences: \n",
    "                                                           [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "\n",
    "#identifying only nouns\n",
    "# for sentence in data['articles']:\n",
    "###     data['nouns'] = [token for token, pos in pos_tag(word_tokenize(sentence)) if pos.startswith('N')]\n",
    "\n",
    "## NLTK lemmatizing and pos tagging\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizing each word with its POS tag, in each sentence\n",
    "para_df_Resume_exp['tokens_sentences_lemmatized'] = para_df_Resume_exp['POS_tokens'].map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")\n",
    "\n",
    "para_df_JD['tokens_sentences_lemmatized'] = para_df_JD['POS_tokens'].map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")\n",
    "\n",
    "# stop words\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'like', 'make', 'see', 'want', 'come', 'take', 'use', 'would', 'can']\n",
    "#stopwords_other = ['one', 'mr', 'bbc', 'image', 'getty', 'de', 'en', 'caption', 'also', 'copyright', 'something']\n",
    "my_stopwords = stopwords.words('English') #+ stopwords_verbs + stopwords_other\n",
    "\n",
    "from itertools import chain\n",
    "para_df_Resume_exp['tokens'] = para_df_Resume_exp['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "para_df_Resume_exp['tokens'] = para_df_Resume_exp['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>2])\n",
    "\n",
    "para_df_JD['tokens'] = para_df_JD['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "para_df_JD['tokens'] = para_df_JD['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>2])\n",
    "\n",
    "\n",
    "para_df_Resume_exp['tokens']\n",
    "\n",
    "## looking for by grams and tri grams\n",
    "\n",
    "from gensim.models import Phrases\n",
    "nouns= []\n",
    "tokens = para_df_Resume_exp['tokens'].tolist()\n",
    "# for tok in tokens:\n",
    "#     #print(pos_tag(tok))\n",
    "#     nouns.append([token for token, pos in pos_tag(tok) if pos.startswith('N')])\n",
    "# nouns = list(filter(None, nouns))\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])\n",
    "tokens = list(filter(None, tokens))\n",
    "\n",
    "\n",
    "tokens1 = para_df_JD['tokens'].tolist()\n",
    "# for tok in tokens:\n",
    "#     #print(pos_tag(tok))\n",
    "#     nouns.append([token for token, pos in pos_tag(tok) if pos.startswith('N')])\n",
    "# nouns = list(filter(None, nouns))\n",
    "bigram_model1 = Phrases(tokens1)\n",
    "trigram_model1 = Phrases(bigram_model[tokens1], min_count=1)\n",
    "tokens1 = list(trigram_model[bigram_model[tokens1]])\n",
    "tokens1 = list(filter(None, tokens1))\n",
    "\n",
    "tokens1\n",
    "\n",
    "#### unique words list\n",
    "unique_list = [] \n",
    "for x in tokens:\n",
    "    for y in x:\n",
    "        if y not in unique_list: \n",
    "            unique_list.append(y)\n",
    "unique_list = [w.replace('_', '') for w in unique_list]\n",
    "len(unique_list)\n",
    "\n",
    "\n",
    "#JD\n",
    "unique_listJD = [] \n",
    "for x in tokens1:\n",
    "    for y in x:\n",
    "        if y not in unique_listJD: \n",
    "            unique_listJD.append(y)\n",
    "unique_listJD = [w.replace('_', '') for w in unique_listJD]\n",
    "len(unique_listJD)\n",
    "\n",
    "def chk_lst(wrd,lst):\n",
    "    flg=False\n",
    "    for i in lst:\n",
    "        if re.search(wrd,i):\n",
    "            flg=True\n",
    "            return flg\n",
    "    return flg\n",
    "        \n",
    "\n",
    "def getmetrics(inp, result):\n",
    "    # Regexp match with any string in the list\n",
    "#     tn = len([value for value in inp if chk_lst(value,result)])\n",
    "    tp = len([value for value in inp if chk_lst(value,result)])\n",
    "    fp = len([value for value in inp if not chk_lst(value,result)])\n",
    "    fn = len([value for value in result if not chk_lst(value,inp)])\n",
    "    tp_val = [value for value in inp if chk_lst(value,result)]\n",
    "    print(\"True Positives - %s, False Positives - %s, False Negatives - %s\"%(tp,fp,fn))\n",
    "    if tp>0:\n",
    "#         accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "    else:\n",
    "#         accuracy=0\n",
    "        precision=0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "    return precision,recall,f1,tp_val\n",
    "\n",
    "# Evaluation of Only nltk Tokens - Model 1\n",
    "unique_list_low = [i.lower() for i in unique_list]\n",
    "unique_listJD_low = [i.lower() for i in unique_listJD]\n",
    "m1_precision, m1_recall, m1_f1, mat_val_m1 = getmetrics(unique_listJD_low, unique_list_low)\n",
    "print(\"Model 1 Precision - %s \\nModel 1 Recall - %s \\nModel 1 F1 - %s\"%( m1_precision,m1_recall,m1_f1))\n",
    "\n",
    "if (m1_precision >= 0.3 and float(m1_f1) >= 0.3) :\n",
    "    Print(\"Best\")\n",
    "    Rel_Exp.append('High')\n",
    "elif (m1_precision >= 0.2 and float(m1_f1) >= 0.2) :\n",
    "    print(\"Good\")\n",
    "    Rel_Exp.append('Moderate')\n",
    "else:\n",
    "    print(\"Nakli\")\n",
    "    Rel_Exp.append('Low')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masters_list = ['pgdm','mba','msc','md','md','mca']\n",
    "batchelors_list = ['bsc','bba','ba','be','btech','barch']\n",
    "\n",
    "tier1 = file['university_name'].tolist()\n",
    "tier1\n",
    "\n",
    "\n",
    "tok = tokens\n",
    "\n",
    "\n",
    "degree = []\n",
    "for i in tok:\n",
    "    if set(masters_list).intersection(i) != set():\n",
    "        degree.append('masters')\n",
    "    elif set(batchelors_list).intersection(i) != set():\n",
    "        degree.append('bachelors')\n",
    "    else:\n",
    "        degree.append(\"others\")\n",
    "        \n",
    "        \n",
    "        \n",
    "mystring = 'university of delhi'\n",
    "s = mystring.replace(\" \", \"_\")\n",
    "s\n",
    "\n",
    "\n",
    "tier1_list = []\n",
    "for i in tier1:\n",
    "    s = i.replace(\" \",\"_\")\n",
    "    tier1_list.append(s.lower())\n",
    "print(tier1_list)\n",
    "\n",
    "\n",
    "\n",
    "tier1_match = []\n",
    "for i in tok:\n",
    "    if set(tier1_list).intersection(i) != set():\n",
    "        tier1_match.append('masters')\n",
    "    elif set(tier1_list).intersection(i) != set():\n",
    "        tier1_match.append('bachelors')\n",
    "    else:\n",
    "        tier1_match.append(\"others\")\n",
    "        \n",
    "        \n",
    "tier1india = pd.read_csv('tier1india.csv')\n",
    "tier2india = pd.read_csv('tier2india.csv')\n",
    "tier3india = pd.read_csv('tier3india.csv')\n",
    "\n",
    "tier1india_list = tier1india['univ'].tolist()\n",
    "tier2india_list = tier2india['univ'].tolist()\n",
    "tier3india_list = tier3india['univ'].tolist()\n",
    "\n",
    "\n",
    "tier1_list_india = []\n",
    "for i in tier1india_list:\n",
    "    s = i.replace(\" \",\"_\")\n",
    "    tier1_list_india.append(s.lower())\n",
    "print(tier1_list_india)\n",
    "\n",
    "tier2_list_india = []\n",
    "for i in tier2india_list:\n",
    "    s = i.replace(\" \",\"_\")\n",
    "    tier2_list_india.append(s.lower())\n",
    "print(tier2_list_india)\n",
    "\n",
    "\n",
    "tier3_list_india = []\n",
    "for i in tier3india_list:\n",
    "    s = i.replace(\" \",\"_\")\n",
    "    tier3_list_india.append(s.lower())\n",
    "print(tier3_list_india)\n",
    "\n",
    "\n",
    "tier_india_match = []\n",
    "for i in tok:\n",
    "    if set(tier1_list_india).intersection(i) != set():\n",
    "        tier_india_match.append('Tier1')\n",
    "    elif set(tier2_list_india).intersection(i) != set():\n",
    "        tier_india_match.append('Tier2')\n",
    "    elif set(tier3_list_india).intersection(i) != set():\n",
    "        tier_india_match.append('Tier3')\n",
    "    else:\n",
    "        tier_india_match.append(\"others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(Resume_Names, columns =['ResumeName'])\n",
    "\n",
    "data['Total Experience']=tot_exp\n",
    "data['Relevant Experience']=Rel_Exp\n",
    "data['Skill Level']=skill_level\n",
    "data['College Tier']=tier\n",
    "data['Degree']=degree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(s):\n",
    "    if s=='High':\n",
    "        return 2\n",
    "    elif s=='Moderate':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def converter_tier(s):\n",
    "    if s=='Tier 1':\n",
    "        return 2\n",
    "    elif s=='Tier 2':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def converter_edu(s):\n",
    "    if s=='Masters':\n",
    "        return 2\n",
    "    elif s=='Bachelors':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['Relevant_Experience']=data['Relevant Experience'].apply(converter)\n",
    "data['Skill_Level']=data['Skill Level'].apply(converter)\n",
    "data['College_Tier']=data['College Tier'].apply(converter_tier)\n",
    "data['degree']=data['Degree'].apply(converter_edu)\n",
    "\n",
    "feature_cols = ['Total Experience', 'Relevant_Experience', 'Skill_Level', 'College_Tier','degree',]\n",
    "X = data[feature_cols] # Features\n",
    "y = data.Label # Target variable\n",
    "\n",
    "#Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "#Predict the response for theEW dataset\n",
    "new_pred = clf.predict(X_new)\n",
    "\n",
    "print(y_pred)\n",
    "print(X_test)\n",
    "print(X_new)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(new_pred)):\n",
    "    if new_pred[i]==1:\n",
    "        print('The Candidate is Accepted')\n",
    "    else:\n",
    "        print('The Candidate is Rejected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Candidate is Rejected\n",
    "The Candidate is Rejected\n",
    "The Candidate is Rejected\n",
    "The Candidate is Accepted\n",
    "The Candidate is Rejected\n",
    "The Candidate is Rejected\n",
    "The Candidate is Rejected\n",
    "The Candidate is Accepted\n",
    "The Candidate is Rejected\n",
    "The Candidate is Rejected\n",
    "The Candidate is Rejected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
